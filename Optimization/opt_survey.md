### Blogs

1. [Natural Gradient Descent (Agustinus Kristiadi)](https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/)
2. [Optimizing with constraints: reparametrization and geometry (VLAD NICULAE).](https://vene.ro/blog/mirror-descent.html)
3. [Derivation of KL divergence by Bregman divergence (Kento Nozawa)](https://nzw0301.github.io/2018/04/KL_as_Bregman)
4. [First-Order Optimization Algorithms for Machine Learning - Projected-Gradient Methods (Mark Schmidt)](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S20/S5.pdf)
5. [Gradient Surgery for Multi-Task Learning (Yu et al.)](https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)|[code](https://github.com/Cranial-XIX/CAGrad/blob/main/toy.py)
