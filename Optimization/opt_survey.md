### Blogs

1. [Natural Gradient Descent (Agustinus Kristiadi)](https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/)
2. [Projected Gradient Descent, Optimizing with constraints: reparametrization and geometry (VLAD NICULAE).](https://vene.ro/blog/mirror-descent.html)
3. [Derivation of KL divergence by Bregman divergence (Kento Nozawa)](https://nzw0301.github.io/2018/04/KL_as_Bregman)
4. [First-Order Optimization Algorithms for Machine Learning - Projected-Gradient Methods (Mark Schmidt)](https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S20/S5.pdf)


### MTL:
1. [Gradient Surgery for Multi-Task Learning (Yu et al.)](https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)|[code](https://github.com/Cranial-XIX/CAGrad/blob/main/toy.py)
2. [Multi-Task Learning with User Preferences](http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf)|[code](https://github.com/dbmptr/EPOSearch)|[video](https://www.youtube.com/watch?v=mgxrjGw6WKU)|

### Papers:
1. [Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum (Zeke Xie)](https://proceedings.mlr.press/v162/xie22d/xie22d.pdf)|[blog1](https://www.zhihu.com/question/323747423/answer/2576604040)|[blog2](https://www.zhihu.com/question/68109802/answer/1677007564)|[blog3](https://www.zhihu.com/question/42115548/answer/1636798770)|[blog4](zhihu.com/question/395685065/answer/2535950728)
2. [A DIFFUSION THEORY FOR DEEP LEARNING DYNAMICS: STOCHASTIC GRADIENT DESCENT EXPONENTIALLY FAVORS FLAT MINIMA (Zeke Xie)](https://arxiv.org/pdf/2002.03495.pdf)


### Schloars:
1. [Francis Bach](https://www.di.ens.fr/~fbach/)
